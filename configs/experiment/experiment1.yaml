# @package _global_

# to execute this experiment run:
# python run.py experiment=example_full.yaml

defaults:
  - override /trainer: null # override trainer to null so it's not loaded from main config defaults...
  - override /model: concat_fasttext
  - override /datamodule: hateful_memes_datamodule
  - override /callbacks: null
  - override /logger: tensorboard

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place

seed: 12345

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 1
  min_epochs: 1
  max_epochs: 20
  gradient_clip_val: 0.5
  weights_summary: full
  # resume_from_checkpoint: ${work_dir}/last.ckpt

model:
  _target_: src.models.LanguageAndVisionConcat
  embedding_dim: 100
  model_type: concat
  language_feature_dim: 256
  backbone_output_dim: 1000
  vision_feature_dim: 256
  fusion_output_size: 256
  dropout_p: 0.1


callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/acc"
    save_top_k: 2
    save_last: True
    mode: "max"
    dirpath: "checkpoints/"
    filename: "concat-bert-{epoch:02d}"
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "val/acc"
    patience: 10
    mode: "max"
